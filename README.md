# Micrograd - Manual Backpropagation ğŸ§®

A minimal **autograd engine** built from scratch in Python, inspired by [Karpathyâ€™s micrograd](https://github.com/karpathy/micrograd).  
This project demonstrates how **backpropagation** works under the hood without relying on deep learning frameworks.

---

## ğŸš€ Features
- Tiny **computational graph** for scalar values  
- **Forward pass** for mathematical operations  
- **Backward pass** with manual backpropagation  
- Implemented in **pure Python** (no TensorFlow/PyTorch)  
- Designed and tested in **Google Colab**  

---

## ğŸ”§ Tech Stack
- Python 3.x  
- Jupyter / Google Colab  
- NumPy (optional for experiments)  

---

## ğŸ“‚ Repository Structure
micrograd-project/
â”‚â”€â”€ Micrograd.ipynb # Colab notebook with implementation + experiments
â”‚â”€â”€ README.md # Project documentation
â”‚â”€â”€ requirements.txt # Dependencies (optional)

yaml
Copy
Edit

---

## ğŸ“˜ How It Works
- Define scalar values as **nodes** in a computation graph.  
- Perform operations (add, multiply, etc.) while building the graph.  
- Run **forward propagation** to compute results.  
- Call `.backward()` to apply **chain rule of differentiation** and compute gradients.  

---

## â–¶ï¸ Run the Project

### ğŸ”— Run in Google Colab
Click below to open the notebook directly in Google Colab:  

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/YourUsername/micrograd-project/blob/main/Micrograd.ipynb)

*(Replace `YourUsername` with your GitHub username)*

### ğŸ’» Run Locally
```bash
git clone https://github.com/YourUsername/micrograd-project.git
cd micrograd-project
pip install -r requirements.txt
jupyter notebook Micrograd.ipynb
ğŸ“– References
Micrograd by Andrej Karpathy

ğŸ‘©â€ğŸ’» Author
Spandana Ray

yaml
Copy
Edit
